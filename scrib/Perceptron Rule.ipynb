{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a single perceptron with $n$ inputs, and let $ x_i $ and $w_i$ be the $i_{th}$ input and weight of the perceptron. Then the learning algorithm can be expressed as, \n",
    "\n",
    "$$ \\hat{y} = \\sum_{i} w_i \\cdot {x_i} $$\n",
    "\n",
    "$$ \\delta_{w_i} = \\eta \\cdot (y - \\hat{y}) \\cdot x_i $$\n",
    "\n",
    "$$ w_i = w_i + \\delta_{w_i} $$\n",
    "\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "My question concerns the second of the above equation. While I can understand what it is doing by considering the mathematical properties, I found it unintuitive to multiply the $x_i$. What does it actually mean by multiply the prediction error with the input ? Also, the appropriate $\\eta$ depends on the magnitude of the inputs. Afterall 0.01 times 100 is the same as 1 times 1.\n",
    "\n",
    "After some pounding, I am wondering whether it will be a lot more intuitive if I modified the second equation to.\n",
    "\n",
    "$$ \\delta_{w_i} = \\eta \\cdot \\frac{1}{n} \\cdot \\frac{y - \\hat{y}}{x_i} $$ \n",
    "\n",
    "Please allow me to elaborate. \n",
    "\n",
    "Let ignore the learning rate $\\eta$ for the time being. The difference between the label and prediction is given by $y - \\hat{y}$. If $n=1$, that's when we only have one input, then $\\frac{y - \\hat{y}}{x_i}$ will be the exact amount that we need to adjust in order for $ y = \\hat{y}$. When $n > 1$ we can simply adjusted the term by multiplying $\\frac{1}{n}$ to make $y = \\hat{y}$, so each input contributes the same amount toward the goal.\n",
    "\n",
    "And the rational behind $\\eta$ can be understand as to avoid overfiting the model. Without the $\\eta$, our model would be able to perfectly predict the single observable that we have used to train the perceptron. But this would most likely to an overfit, so we only want to move a small step toward that direction, hence the learning rate.\n",
    "\n",
    "I have check over multiple reference. It seems the equation represented in the course is the standard one. So I am wonder if there is any flaw in my thinking ?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-12   1]\n",
      "[0 1]\n",
      "-25\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "w_2 = np.array([2, -1])\n",
    "\n",
    "w_1 = np.array(\n",
    "    [[1, 1, -5],\n",
    "     [3, -4, 2]]\n",
    ")\n",
    "x = np.array([1,2,3])\n",
    "\n",
    "print np.dot(w_1, x)\n",
    "actv_func = np.vectorize(lambda x: 1 if x>0 else 0)\n",
    "print actv_func(np.dot(w_1, x))\n",
    "print np.dot(w_2, (np.dot(w_1, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 1), (2, 3), (3,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_2.shape, w_1.shape, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2, 15]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(\n",
    "    np.array([[1,2,-1]]),\n",
    "    np.array([[3,2], [-1,4], [3,-5]]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
